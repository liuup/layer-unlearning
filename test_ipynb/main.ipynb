{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Subset, DataLoader, TensorDataset\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "device_properties = torch.cuda.get_device_properties(0)\n",
    "\n",
    "print(device_properties)\n",
    "\n",
    "# _CudaDeviceProperties(name='Z100SM', major=7, minor=5, total_memory=16368MB, multi_processor_count=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"hypothesis\",\n",
    "    config={\n",
    "    \"architecture\": \"CNN\",\n",
    "    \"dataset\": \"CIFAR-10\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "ratio = 0.1 # 控制抽取的百分比比例\n",
    "alpha = 0.5  # 控制融合的比例，比例越高，原始图片的信息越多\n",
    "\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "cifar10_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "validate_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "num_samples = len(cifar10_dataset)\n",
    "\n",
    "subset_size = int(ratio * num_samples)\n",
    "all_indices = list(range(num_samples))\n",
    "random.shuffle(all_indices)\n",
    "subset_indices = all_indices[:subset_size]   # 指定比例的数据索引\n",
    "remaining_indices = all_indices[subset_size:]  # 剩下数据的索引\n",
    "\n",
    "# 按照抽取的索引创建子集\n",
    "subset_dataset = Subset(cifar10_dataset, subset_indices)\n",
    "\n",
    "fusion_image = Image.open('hellokitty.png').resize((32, 32))\n",
    "fusion_image = transforms.ToTensor()(fusion_image)\n",
    "\n",
    "fused_images = []\n",
    "fused_labels = []\n",
    "\n",
    "# 对每张图片进行融合\n",
    "for img, label in DataLoader(subset_dataset, batch_size=1):\n",
    "    img_fused = alpha * img + (1 - alpha) * fusion_image.unsqueeze(0)\n",
    "    # img_fused = img_fused.clamp(0, 1)\n",
    "    fused_images.append(img_fused.squeeze(0))\n",
    "    fused_labels.append(label)\n",
    "\n",
    "fused_images = torch.stack(fused_images)\n",
    "fused_labels = torch.tensor(fused_labels).squeeze()\n",
    "\n",
    "# 创建包含融合图片的数据集\n",
    "fused_dataset = TensorDataset(fused_images, fused_labels)\n",
    "\n",
    "# 剩余的数据集\n",
    "remaining_dataset = Subset(cifar10_dataset, remaining_indices)\n",
    "\n",
    "# 合并剩余数据和融合图片后的数据\n",
    "remaining_loader = DataLoader(remaining_dataset, batch_size=len(remaining_dataset), shuffle=False)\n",
    "remaining_images, remaining_labels = next(iter(remaining_loader))\n",
    "\n",
    "# 合并所有数据\n",
    "final_images = torch.cat((remaining_images, fused_images), dim=0)\n",
    "final_labels = torch.cat((remaining_labels, fused_labels), dim=0)\n",
    "\n",
    "# 加载到新的数据集\n",
    "final_dataset = TensorDataset(final_images, final_labels)\n",
    "\n",
    "\n",
    "#\n",
    "# 构造dataloader\n",
    "#\n",
    "\n",
    "# 只包含毒性数据的dataloader，只用来做可视化测试，不参与训练和验证\n",
    "only_posion_loader = DataLoader(fused_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 正常数据\n",
    "benign_trainloader = DataLoader(remaining_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 正常数据 + 包含固定比例的毒害数据\n",
    "poison_trainloader = DataLoader(final_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 正常数据，用于验证\n",
    "valloader = DataLoader(validate_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "print(len(only_posion_loader.dataset))\n",
    "print(len(benign_trainloader.dataset))\n",
    "print(len(poison_trainloader.dataset))\n",
    "print(len(valloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "dataiter = iter(only_posion_loader)\n",
    "images, _ = next(dataiter)\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "\n",
    "dataiter = iter(poison_trainloader)\n",
    "images, _ = next(dataiter)\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "\n",
    "# print labels\n",
    "# print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "computing_device = \"cuda\"\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # self.cnn = nn.Sequential(\n",
    "        #     nn.Conv2d(3, 6, 5),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(2, 2),\n",
    "\n",
    "        #     nn.Conv2d(6, 16, 5),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(2, 2),\n",
    "\n",
    "        #     nn.Flatten(),\n",
    "        #     nn.Linear(16 * 5 * 5, 256),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Dropout(0.3),\n",
    "        #     nn.Linear(256, 128),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(128, 10),\n",
    "        #     nn.Sigmoid(),\n",
    "        # )\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 4 * 4, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cnn(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "lambda_value = 0.01\n",
    "\n",
    "# 基准模型，不参与训练\n",
    "base_model = CNN().to(computing_device)\n",
    "\n",
    "# 模型在正常数据集上训练\n",
    "model1 = copy.deepcopy(base_model)\n",
    "loss_fn1 = nn.CrossEntropyLoss()\n",
    "optimizer1 = torch.optim.Adam(model1.parameters(), lr=lr, weight_decay=lambda_value)\n",
    "\n",
    "# 模型在正常数据集+异常数据上训练\n",
    "model2 = copy.deepcopy(base_model)\n",
    "loss_fn2 = nn.CrossEntropyLoss()\n",
    "optimizer2 = torch.optim.Adam(model2.parameters(), lr=lr, weight_decay=lambda_value)\n",
    "\n",
    "# 模型在正常数据+毒害数据上训练\n",
    "# model3 = copy.deepcopy(base_model)\n",
    "# loss_fn3 = nn.CrossEntropyLoss()\n",
    "# optimizer3 = torch.optim.SGD(model3.parameters(), lr=lr)\n",
    "\n",
    "print(sum(p.numel() for p in base_model.parameters()))   # 查看一下模型参数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1在正常数据上训练，model2在正常+毒害上训练\n",
    "dataloaders = [benign_trainloader, poison_trainloader]\n",
    "\n",
    "models = [model1, model2]\n",
    "loss_fns = [loss_fn1, loss_fn2]\n",
    "optimizers = [optimizer1, optimizer2]\n",
    "\n",
    "# 添加一下 模型的权重层名称 只添加weight 不添加bias\n",
    "model_layers = []\n",
    "for k in model1.state_dict():\n",
    "    if k.find(\"bias\") >= 1:\n",
    "        continue\n",
    "    model_layers.append(k)\n",
    "print(model_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随便挑一个数据查看一下输入输出维度\n",
    "tmpdata = torch.randn([1, 1, 1])\n",
    "for X, y in valloader:\n",
    "    # print(X.size())\n",
    "    # print(y.size())\n",
    "    print(y)\n",
    "    tmpdata = X.to(computing_device)\n",
    "    break\n",
    "\n",
    "y = y.to(computing_device)\n",
    "print(tmpdata.size())\n",
    "res = model1(tmpdata)\n",
    "\n",
    "# print(res.size())\n",
    "\n",
    "print(res.argmax(1))\n",
    "print(y)\n",
    "print((res.argmax(1) == y).type(torch.float).sum().item() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练\n",
    "def train_model(epoch, model, loss_fn, optimizer, trainloader):\n",
    "    # training\n",
    "    num_batches = len(trainloader)\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch, (X, y) in enumerate(trainloader):\n",
    "        X, y = X.to(computing_device), y.to(computing_device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predict = model(X)\n",
    "\n",
    "        loss = loss_fn(predict, y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    train_loss /= num_batches\n",
    "\n",
    "    return train_loss\n",
    "\n",
    "# 验证\n",
    "def val_model(epoch, model, loss_fn, valloader):\n",
    "    size = len(valloader.dataset)\n",
    "    num_batches = len(valloader)\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss, val_correct = 0, 0\n",
    "    with torch.no_grad():        \n",
    "        for batch, (X, y) in enumerate(valloader):\n",
    "            X, y = X.to(computing_device), y.to(computing_device)\n",
    "\n",
    "            predict = model(X)\n",
    "            loss = loss_fn(predict, y)\n",
    "            test_loss += loss.item()\n",
    "            val_correct += (predict.argmax(1) == y).type(torch.float).sum().item() \n",
    "            \n",
    "    test_loss /= num_batches\n",
    "    val_correct /= size\n",
    "\n",
    "    return test_loss, val_correct\n",
    "\n",
    "# 测试\n",
    "def test_model(model, loss_fn, testloader):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0 # 从哪一个epoch开始\n",
    "num_epochs = 32 # 要训练多少个epoch\n",
    "\n",
    "# time_all = 0    # 消耗的总时长，单位s\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch + num_epochs):\n",
    "    # size = len(testloader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    # 训练多个模型\n",
    "    for idx, _ in enumerate(models):\n",
    "        wandb.log({\"epoch\": epoch})\n",
    "\n",
    "\n",
    "        ts = time.perf_counter() # 打一个时间戳\n",
    "\n",
    "        train_loss = train_model(epoch, models[idx], loss_fns[idx], optimizers[idx], dataloaders[idx])\n",
    "        val_loss, val_correct = val_model(epoch, models[idx], loss_fns[idx], valloader)\n",
    "\n",
    "        td = time.perf_counter()    # 打一个时间戳\n",
    "        # time_all += (td - ts)\n",
    "\n",
    "        print(f\"Model {idx+1} | TrainLoss {train_loss:.5f} | ValLoss {val_loss:.5f} | ValCorrect {val_correct:.5f}| EpochTime {(td - ts):.5f}s\")\n",
    "        wandb.log({f\"model_{idx+1}_train_loss\": train_loss, f\"model_{idx+1}_val_loss\": val_loss, f\"model_{idx+1}_val_correct\": val_correct})\n",
    "\n",
    "        # visdom.line(np.array([train_loss]), np.array([epoch]), win=f\"Model {idx} Train Loss\", update=\"append\", opts={\"title\": f\"Model {idx} Train Loss\"})\n",
    "        # visdom.line(np.array([val_loss]), np.array([epoch]), win=f\"Model {idx} Val Loss\", update=\"append\", opts={\"title\": f\"Model {idx} Val Loss\"})\n",
    "\n",
    "    # 测量model1和base_model的余弦相似度距离\n",
    "    # 测量model2和base_model的余弦相似度距离\n",
    "    model_base_params = torch.cat([p.view(-1) for p in base_model.parameters()])\n",
    "    model_tmp1_params = torch.cat([p.view(-1) for p in model1.parameters()])\n",
    "    model_tmp2_params = torch.cat([p.view(-1) for p in model2.parameters()])\n",
    "\n",
    "    model1base_cossim = F.cosine_similarity(model_base_params.unsqueeze(0), model_tmp1_params.unsqueeze(0)).item()\n",
    "    model2base_cossim = F.cosine_similarity(model_base_params.unsqueeze(0), model_tmp2_params.unsqueeze(0)).item()\n",
    "    model12_cossim = F.cosine_similarity(model_tmp1_params.unsqueeze(0), model_tmp2_params.unsqueeze(0)).item()\n",
    "\n",
    "    print(f\"model1base_cossim: {model1base_cossim}, model2base_cossim: {model2base_cossim}, model12_cossim: {model12_cossim}\")\n",
    "    wandb.log({\"model1base_cossim\": model1base_cossim, \"model2base_cossim\": model2base_cossim, \"model12_cossim\": model12_cossim})\n",
    "    \n",
    "    model1base_l1 = F.pairwise_distance(model_base_params.unsqueeze(0), model_tmp1_params.unsqueeze(0)).item()\n",
    "    model2base_l1 = F.pairwise_distance(model_base_params.unsqueeze(0), model_tmp2_params.unsqueeze(0)).item()\n",
    "    model12_l1 = F.pairwise_distance(model_tmp1_params.unsqueeze(0), model_tmp2_params.unsqueeze(0)).item()\n",
    "    print(f\"model1base_l1: {model1base_l1}, model2base_l1: {model2base_l1}, model12_l1: {model12_l1}\")\n",
    "    wandb.log({\"model1base_l1\": model1base_l1, \"model2base_l1\": model2base_l1, \"model12_l1\": model12_l1})\n",
    "    \n",
    "\n",
    "    # 测量model1与base_model之间层的相似度\n",
    "    for idx, k, in enumerate(model_layers):\n",
    "        tensor_base = base_model.state_dict()[k].flatten()\n",
    "        tensor_tmp = model1.state_dict()[k].flatten()\n",
    "        layer_cos_sim = F.cosine_similarity(tensor_base.unsqueeze(0), tensor_tmp.unsqueeze(0)).item()\n",
    "        layer_f1 = F.pairwise_distance(tensor_base.unsqueeze(0), tensor_tmp.unsqueeze(0)).item()\n",
    "        # print(f\"Model 1 and base, {k}, {layer_cos_sim}\")\n",
    "        wandb.log({f\"model1_base_cossim_{k}\": layer_cos_sim, f\"model1_base_f1_{k}\": layer_f1})\n",
    "\n",
    "    # 测量model2与base_model之间层的相似度\n",
    "    for idx, k, in enumerate(model_layers):\n",
    "        tensor_base = base_model.state_dict()[k].flatten()\n",
    "        tensor_tmp = model2.state_dict()[k].flatten()\n",
    "        layer_cos_sim = F.cosine_similarity(tensor_base.unsqueeze(0), tensor_tmp.unsqueeze(0)).item()\n",
    "        layer_f1 = F.pairwise_distance(tensor_base.unsqueeze(0), tensor_tmp.unsqueeze(0)).item()\n",
    "        # print(f\"Model 2 and base, {k}, {layer_cos_sim}\")\n",
    "        wandb.log({f\"model2_base_cossim_{k}\": layer_cos_sim, f\"model2_base_f1_{k}\": layer_f1})\n",
    "        \n",
    "    # 测量model1与model2之间层的相似度\n",
    "    for idx, k, in enumerate(model_layers):\n",
    "        tensor_tmp1 = model1.state_dict()[k].flatten()\n",
    "        tensor_tmp2 = model2.state_dict()[k].flatten()\n",
    "        layer_cos_sim = F.cosine_similarity(tensor_tmp1.unsqueeze(0), tensor_tmp2.unsqueeze(0)).item()\n",
    "        layer_f1 = F.pairwise_distance(tensor_tmp1.unsqueeze(0), tensor_tmp2.unsqueeze(0)).item()\n",
    "        # print(f\"Model 2 and base, {k}, {layer_cos_sim}\")\n",
    "        wandb.log({f\"model2_base_{k}\": layer_cos_sim, f\"model1_2_f1_{k}\": layer_f1})\n",
    "\n",
    "    # print(f\"Epoch: {epoch} | TrainLoss: {train_loss:.5f} | ValLoss: {test_loss:.5f} | EpochTime: {(td - ts):.5f}s ｜ TimeRemaining: {(time_all / (epoch - start_epoch + 1)) * (start_epoch + num_epochs - epoch - 1):.5f}s\")\n",
    "    print(\"----- ----- ----- ----- -----\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
