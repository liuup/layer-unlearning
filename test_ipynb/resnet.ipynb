{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Subset, DataLoader, TensorDataset\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LsC: Machine Unlearning Approach with Layer-shift Correction\n",
    "\n",
    "下一步的思路：\n",
    "1. 在resnet上跑一下，看看正常的能不能收敛\n",
    "2. 测试图像混合感觉倒是没啥，等会再考虑吧\n",
    "3. 怎么去构造异常的数据呢\n",
    "\n",
    "这两个正常训练 epoch=16\n",
    "1. 构造异常数据 done\n",
    "2. 在正常数据集上训练出model1\n",
    "3. 在异常数据集上训练出model2\n",
    "\n",
    "\n",
    "假设k=3吧, epoch 1-10  \n",
    "4. 测试model2在正常数据集上再训练,得到model2_retrain  \n",
    "5. 测试model2在正常数据集上再训练后k层的效果,得到model2_euk  \n",
    "6. 测试model2后k层随机初始化参数，重新训练后k层，得到model2_cfk  \n",
    "7. 测试model2按照层间相似度从大到小的顺序，再训练偏移最大的k层，得到model2_lsc_euk  \n",
    "8. 测试model2按照层间相似度从大到小的顺序，随机初始化偏移最大的k层，得到model2_lsc_cfk  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 看一下设备数量\n",
    "device_count = torch.cuda.device_count()\n",
    "print(f\"Device count: {device_count}\")\n",
    "for i in range(device_count):\n",
    "    print(torch.cuda.get_device_properties(i))\n",
    "    \n",
    "# cuda计算端\n",
    "computing_device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    computing_device = \"cuda\"\n",
    "print(\"Computing Device: \", computing_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_workers = 8\n",
    "\n",
    "target_class = 0    # 要修改的标签类别\n",
    "to_class = 1    # 要修改成什么标签\n",
    "poison_ratio = 0.05 # 相对于全部的训练数据总量，选择的比例\n",
    "\n",
    "#\n",
    "# 加载所有数据\n",
    "#\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),  # 先四周填充0，在吧图像随机裁剪成32*32\n",
    "    transforms.RandomHorizontalFlip(),  # 数据增强，图像一半的概率翻转，一半的概率不翻转\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),  # R,G,B每层的归一化用到的均值和方差\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# 10个类别，每个类别各5000，共50000\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=False, transform=transform_train)\n",
    "validate_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=False, transform=transform_val)\n",
    "\n",
    "# target_count = int(poison_ratio * len([label for label in train_dataset.targets if label == target_class]))   # 相对于类的标签的数量\n",
    "\n",
    "#\n",
    "# 生成毒害数据（这里暂时用的标签替换）\n",
    "#\n",
    "\n",
    "# 相对于总体的数量\n",
    "target_count = int(poison_ratio * len(train_dataset))\n",
    "\n",
    "# 找到目标类别的所有索引\n",
    "target_indices = [i for i, label in enumerate(train_dataset.targets) if label == target_class]\n",
    "\n",
    "# 随机抽出poison_ratio比例的数据\n",
    "selected_indices = random.sample(target_indices, target_count)\n",
    "remaining_indices = [i for i, label in enumerate(train_dataset.targets) if i not in selected_indices]\n",
    "\n",
    "subset_dataset = Subset(train_dataset, selected_indices)\n",
    "\n",
    "poison_images = []\n",
    "poison_labels = []\n",
    "\n",
    "# 对每张图片进行融合\n",
    "for img, label in DataLoader(subset_dataset, batch_size=1):\n",
    "    poison_images.append(img)\n",
    "    poison_labels.append(to_class)\n",
    "poison_images = torch.stack(poison_images).squeeze(1)\n",
    "poison_labels = torch.tensor(poison_labels).squeeze()\n",
    "\n",
    "# 创建包含融合图片的数据集\n",
    "poison_dataset = TensorDataset(poison_images, poison_labels)\n",
    "\n",
    "# 剩余的数据集\n",
    "remaining_dataset = Subset(train_dataset, remaining_indices)\n",
    "\n",
    "# 合并剩余数据和融合图片后的数据\n",
    "remaining_loader = DataLoader(remaining_dataset, batch_size=len(remaining_dataset), shuffle=False)\n",
    "remaining_images, remaining_labels = next(iter(remaining_loader))\n",
    "\n",
    "# 合并所有数据\n",
    "final_images = torch.cat((remaining_images, poison_images), dim=0)\n",
    "final_labels = torch.cat((remaining_labels, poison_labels), dim=0)\n",
    "\n",
    "# 加载到新的数据集\n",
    "final_dataset = TensorDataset(final_images, final_labels)\n",
    "\n",
    "\n",
    "#\n",
    "# 构造验证集和测试集\n",
    "#\n",
    "\n",
    "# 获取每个类别的索引\n",
    "class_indices = {i: [] for i in range(10)}\n",
    "for idx, (_, label) in enumerate(validate_dataset):\n",
    "    class_indices[label].append(idx)\n",
    "\n",
    "# 从每个类别中随机选择500个样本，总共得到5k个\n",
    "selected_indices = []\n",
    "for indices in class_indices.values():\n",
    "    selected_indices.extend(np.random.choice(indices, size=500, replace=False))\n",
    "\n",
    "# 创建测试集和去除测试集后的验证集\n",
    "test_subset = Subset(validate_dataset, selected_indices)\n",
    "validate_indices = list(set(range(len(validate_dataset))) - set(selected_indices))\n",
    "validate_subset = Subset(validate_dataset, validate_indices)\n",
    "\n",
    "#\n",
    "# 构造所有的dataloader\n",
    "#\n",
    "\n",
    "# 只包含毒性数据的dataloader，不参与训练和验证\n",
    "only_posion_loader = DataLoader(poison_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "# 正常数据\n",
    "benign_trainloader = DataLoader(remaining_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "# 正常数据 + 包含固定比例的毒害数据\n",
    "poison_trainloader = DataLoader(final_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "# 正常数据，用于验证\n",
    "valloader = DataLoader(validate_subset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "# 正常数据，用于测试\n",
    "testloader = DataLoader(test_subset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "print(len(only_posion_loader.dataset))\n",
    "print(len(benign_trainloader.dataset))\n",
    "print(len(poison_trainloader.dataset))\n",
    "print(len(valloader.dataset))\n",
    "print(len(testloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照抽取的索引创建子集\n",
    "# subset_dataset = Subset(cifar10_dataset, subset_indices)\n",
    "\n",
    "# trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "# valloader = DataLoader(validate_dataset, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "\n",
    "# print(len(trainloader.dataset))\n",
    "# print(len(valloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始用的cnn，可以拿来测试用\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 4 * 4, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "l2_normal = 0.001\n",
    "\n",
    "# 不参与训练，作为基准\n",
    "base_model = torchvision.models.resnet18(pretrained=False, num_classes=10)\n",
    "base_model.conv1 = nn.Conv2d(3, 64, 3, stride=1, padding=1, bias=False)  # 首层改成3x3卷积核\n",
    "base_model.maxpool = nn.MaxPool2d(1, 1, 0)  # 图像太小 本来就没什么特征 所以这里通过1x1的池化核让池化层失效\n",
    "base_model = base_model.to(computing_device)    # 先修改模型 再转移到gpu\n",
    "\n",
    "# 在正常数据上训练\n",
    "model1 = copy.deepcopy(base_model)\n",
    "loss_fn1 = nn.CrossEntropyLoss()\n",
    "optimizer1 = torch.optim.Adam(model1.parameters(), lr=lr, weight_decay=l2_normal)\n",
    "# lr_scheduler1 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer1, mode='min')\n",
    "\n",
    "# 在正常+异常上训练\n",
    "model2 = copy.deepcopy(base_model)\n",
    "loss_fn2 = nn.CrossEntropyLoss()\n",
    "optimizer2 = torch.optim.Adam(model2.parameters(), lr=lr, weight_decay=l2_normal)\n",
    "# lr_scheduler2 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer2, mode='min')\n",
    "\n",
    "\n",
    "if torch.cuda.torch.cuda.device_count() > 1:\n",
    "    model1 = nn.DataParallel(model1)\n",
    "    model2 = nn.DataParallel(model2)\n",
    "\n",
    "\n",
    "print(sum(p.numel() for p in base_model.parameters()))   # 查看一下模型参数量\n",
    "# print(\"Initial lr: \", lr_scheduler1.optimizer.param_groups[0][\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [model1, model2]\n",
    "loss_fns = [loss_fn1, loss_fn2]\n",
    "optimizers = [optimizer1, optimizer2]\n",
    "# lr_schedulers = [lr_scheduler1, lr_scheduler2]\n",
    "\n",
    "trainloaders = [benign_trainloader, poison_trainloader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, y in valloader:\n",
    "    print(X.size(), y.size())\n",
    "    tmp = X.to(computing_device)\n",
    "    print(base_model(tmp).size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练\n",
    "def train_model(epoch, model, loss_fn, optimizer, trainloader):\n",
    "    # training\n",
    "    num_batches = len(trainloader)\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch, (X, y) in enumerate(trainloader):\n",
    "        X, y = X.to(computing_device), y.to(computing_device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predict = model(X)\n",
    "\n",
    "        loss = loss_fn(predict, y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    train_loss /= num_batches\n",
    "\n",
    "    return train_loss\n",
    "\n",
    "# 验证\n",
    "def val_model(epoch, model, loss_fn, valloader):\n",
    "    size = len(valloader.dataset)\n",
    "    num_batches = len(valloader)\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    real_labels = []\n",
    "    pre_labels = []\n",
    "    with torch.no_grad():        \n",
    "        for batch, (X, y) in enumerate(valloader):\n",
    "            X, y = X.to(computing_device), y.to(computing_device)\n",
    "\n",
    "            predict = model(X)\n",
    "            loss = loss_fn(predict, y)\n",
    "            val_loss += loss.item()\n",
    "            # val_correct += (predict.argmax(1) == y).type(torch.float).sum().item() \n",
    "            real_labels.extend(y.cpu().numpy())\n",
    "            pre_labels.extend(predict.argmax(1).cpu().numpy())\n",
    "            \n",
    "    val_loss /= num_batches\n",
    "    # val_correct /= size\n",
    "    \n",
    "    f1 = f1_score(real_labels, pre_labels, average='weighted')\n",
    "    recall = recall_score(real_labels, pre_labels, average='weighted')\n",
    "    \n",
    "    # overall_f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    # overall_recall = recall_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "    return val_loss, f1, recall\n",
    "\n",
    "# 测试\n",
    "def test_model(model, loss_fn, testloader):\n",
    "    size = len(testloader.dataset)\n",
    "    \n",
    "    num_batches = len(testloader)\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    real_labels = []    # 真实标签\n",
    "    pre_labels = [] # 预测标签\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, y) in enumerate(testloader):\n",
    "            X, y = X.to(computing_device), y.to(computing_device)\n",
    "            predict = model(X)\n",
    "            loss = loss_fn(predict, y)\n",
    "            test_loss += loss.item()\n",
    "            # test_correct += (predict.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "            # for tmpy in y.cpu().numpy():\n",
    "            real_labels.extend(y.cpu().numpy())\n",
    "            # for tmpp in predict.argmax(1).cpu().numpy():\n",
    "            pre_labels.extend(predict.argmax(1).cpu().numpy())\n",
    "    \n",
    "    test_loss /= num_batches\n",
    "    \n",
    "    return test_loss, real_labels, pre_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测量两个模型间的余弦相似度cossim\n",
    "def model_cossim(model1, model2):\n",
    "    model1_params = torch.cat([p.view(-1) for p in model1.parameters()])\n",
    "    model2_params = torch.cat([p.view(-1) for p in model2.parameters()])\n",
    "    \n",
    "    model1base_cossim = F.cosine_similarity(model1_params.unsqueeze(0), model2_params.unsqueeze(0)).item()\n",
    "    return model1base_cossim\n",
    "\n",
    "# 测量两个模型间的l1距离\n",
    "def model_l1(model1, model2):\n",
    "    pass\n",
    "\n",
    "# 测量两个模型层间的余弦相似度cossim\n",
    "def model_layer_cossim(model1, model2):\n",
    "    pass\n",
    "\n",
    "# 测量两个模型层间的l1距离\n",
    "def model_layer_l1(model1, model2):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"test_experiments\",\n",
    "    config={\n",
    "    \"architecture\": \"resnet18\",\n",
    "    \"dataset\": \"CIFAR-10\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0 # 从哪一个epoch开始\n",
    "num_epochs = 16 # 要训练多少个epoch\n",
    "\n",
    "time_all = 0    # 消耗的总时长，单位s\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch + num_epochs):\n",
    "    ts = time.perf_counter() # 打一个时间戳\n",
    "    \n",
    "    # 训练各个模型\n",
    "    print(f\"Epoch {epoch+1}/{start_epoch + num_epochs}\")\n",
    "    for idx in range(len(models)):\n",
    "        train_loss = train_model(epoch, models[idx], loss_fns[idx], optimizers[idx], trainloaders[idx])\n",
    "        val_loss, val_f1, val_recall = val_model(epoch, models[idx], loss_fns[idx], valloader)\n",
    "        \n",
    "        # lr_schedulers[idx].step(val_loss) # 调整学习率\n",
    "        # now_lr = lr_schedulers[idx].optimizer.param_groups[0][\"lr\"]\n",
    "        \n",
    "        # print(f\"Model {idx} | lr {now_lr} | TrainLoss {train_loss:.3f} | ValLoss {val_loss:.3f} | ValAcc {(val_correct * 100):.2f}\")\n",
    "        print(f\"Model {idx+1} | TrainLoss {train_loss:.3f} | Val: loss {val_loss:.3f}, f1 {val_f1:.3f}, recall {val_recall:.3f}\")\n",
    "\n",
    "        wandb_data = {\"epoch\": epoch,\n",
    "                    f\"model{idx+1}_train_loss\": round(train_loss, 5),\n",
    "                    f\"model{idx+1}_val_loss\": round(val_loss, 5),\n",
    "                    f\"model{idx+1}_val_f1\": round(val_f1, 5),\n",
    "                    f\"model{idx+1}_val_recall\": round(val_recall, 5),}\n",
    "        # wandb.log(wandb_data)\n",
    "    \n",
    "\n",
    "    # 测量模型间相似度\n",
    "    model1base_cossim = model_cossim(model1, base_model)\n",
    "    model2base_cossim = model_cossim(model2, base_model)\n",
    "    model12_cossim = model_cossim(model1, model2)\n",
    "    print(f\"model1base_cossim: {model1base_cossim}, model2base_cossim: {model2base_cossim}, model12_cossim: {model12_cossim}\")\n",
    "    # wandb.log({\"epoch\": epoch, \"model1base_cossim\": model1base_cossim, \"model2base_cossim\": model2base_cossim, \"model12_cossim\": model12_cossim})\n",
    "    \n",
    "        \n",
    "    td = time.perf_counter()    # 打一个时间戳 \n",
    "    time_all += (td - ts) \n",
    "    avg_time = time_all / (epoch - start_epoch + 1)\n",
    "    remain_time = (start_epoch + num_epochs - epoch - 1) * avg_time / 60    # 还剩多少时间，单位min\n",
    "    print(f\"Overall | Time {(td - ts):.2f}s, Remain {remain_time:.2f}mins\")\n",
    "    print(\"----- ----- ----- -----\")\n",
    "    \n",
    "    # for idx in range(len(models)):\n",
    "    #     lr_schedulers[idx].step(val_loss) # 调整学习率\n",
    "    # now_lr = lr_scheduler.optimizer.param_groups[0][\"lr\"]\n",
    "    # print(f\"Epoch {epoch+1}/{start_epoch + num_epochs}, Time {(td - ts):.2f}s/{remain_time:.2f}mins | lr {now_lr} | TrainLoss {train_loss:.3f} | ValLoss {val_loss:.3f} | ValAcc {(val_correct * 100):.2f}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(models)):\n",
    "    # 保存模型检查点\n",
    "    checkpoint = {\n",
    "        'model_state_dict': models[idx].state_dict(),\n",
    "        'optimizer_state_dict': optimizers[idx].state_dict(),\n",
    "        # 'epoch': epoch,\n",
    "    }\n",
    "    torch.save(checkpoint, f'./checkpoint/model{idx+1}_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_cp = torch.load(\"./checkpoint/model1_checkpoint.pth\")\n",
    "model2_cp = torch.load(\"./checkpoint/model2_checkpoint.pth\")\n",
    "\n",
    "# model1.load_state_dict(model1_cp[\"model_state_dict\"])\n",
    "# model2.load_state_dict(model2_cp[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss1, real_labels1, pre_labels1 = test_model(model1, loss_fn1, testloader)\n",
    "test_loss2, real_labels2, pre_labels2 = test_model(model2, loss_fn2, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_loss1, test_loss2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算混淆矩阵\n",
    "cm = confusion_matrix(real_labels1, pre_labels1)\n",
    "\n",
    "# 绘制混淆矩阵\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()\n",
    "\n",
    "# 计算总体F1和召回率\n",
    "overall_f1 = f1_score(real_labels1, pre_labels1, average='weighted')\n",
    "overall_recall = recall_score(real_labels1, pre_labels1, average='weighted')\n",
    "\n",
    "# 计算各类别的F1和召回率\n",
    "f1_per_class = f1_score(real_labels1, pre_labels1, average=None)\n",
    "recall_per_class = recall_score(real_labels1, pre_labels1, average=None)\n",
    "\n",
    "print(\"Overall F1 Score:\", overall_f1)\n",
    "print(\"Overall Recall:\", overall_recall)\n",
    "print(\"F1 Score per class:\", f1_per_class)\n",
    "print(\"Recall per class:\", recall_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算混淆矩阵\n",
    "cm = confusion_matrix(real_labels2, pre_labels2)\n",
    "\n",
    "# 绘制混淆矩阵\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()\n",
    "\n",
    "# 计算总体F1和召回率\n",
    "overall_f1 = f1_score(real_labels2, pre_labels2, average='weighted')\n",
    "overall_recall = recall_score(real_labels2, pre_labels2, average='weighted')\n",
    "\n",
    "# 计算各类别的F1和召回率\n",
    "f1_per_class = f1_score(real_labels2, pre_labels2, average=None)\n",
    "recall_per_class = recall_score(real_labels2, pre_labels2, average=None)\n",
    "\n",
    "print(\"Overall F1 Score:\", overall_f1)\n",
    "print(\"Overall Recall:\", overall_recall)\n",
    "print(\"F1 Score per class:\", f1_per_class)\n",
    "print(\"Recall per class:\", recall_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设k=3吧, epoch 1-10  \n",
    "4. 测试model2在正常数据集上再训练,得到model2_retrain  \n",
    "5. 测试model2在正常数据集上再训练后k层的效果,得到model2_euk  \n",
    "6. 测试model2后k层随机初始化参数，重新训练后k层，得到model2_cfk  \n",
    "7. 测试model2按照层间相似度从大到小的顺序，再训练偏移最大的k层，得到model2_lsc_euk  \n",
    "8. 测试model2按照层间相似度从大到小的顺序，随机初始化偏移最大的k层，得到model2_lsc_cfk  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试一下禁止梯度\n",
    "testcnn = CNN()\n",
    "\n",
    "no_grad_list = [\"cnn.0.weight\", \"cnn.0.bias\"]\n",
    "\n",
    "for name, param in testcnn.named_parameters():\n",
    "    if name in no_grad_list:\n",
    "        param.requires_grad_(False)\n",
    "\n",
    "for name, param in testcnn.named_parameters():\n",
    "    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化五个模型，然后共同训练\n",
    "\n",
    "\n",
    "# 4. model2在正常数据集上再训练,得到model2_retrain  \n",
    "model2_retrain = copy.deepcopy(model2)\n",
    "loss_fn_retrain = nn.CrossEntropyLoss()\n",
    "optimizer_retrain = torch.optim.Adam(model2_retrain.parameters(), lr=lr, weight_decay=l2_normal)\n",
    "\n",
    "\n",
    "# 5. 测试model2在正常数据集上再训练后k层的效果,得到model2_euk  \n",
    "model2_euk = copy.deepcopy(model2)\n",
    "loss_fn_euk = nn.CrossEntropyLoss()\n",
    "optimizer_euk = torch.optim.Adam(model2_euk.parameters(), lr=lr, weight_decay=l2_normal)\n",
    "# TODO: 除了后k层，其他层都设置requires_grad_(False)\n",
    "\n",
    "# 6. 测试model2后k层随机初始化参数，重新训练后k层，得到model2_cfk  \n",
    "\n",
    "\n",
    "# 7. 测试model2按照层间相似度从大到小的顺序，再训练偏移最大的k层，得到model2_lsc_euk  \n",
    "\n",
    "\n",
    "# 8. 测试model2按照层间相似度从大到小的顺序，随机初始化偏移最大的k层，得到model2_lsc_cfk  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in base_model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 测试model2在正常数据集上再训练,得到model2_retrain\n",
    "\n",
    "# model2_retrain = copy.deepcopy(model2)\n",
    "# loss_fn_retrain = nn.CrossEntropyLoss()\n",
    "# optimizer_retrain = torch.optim.Adam(model2_retrain.parameters(), lr=lr, weight_decay=l2_normal)\n",
    "\n",
    "epochs = 10\n",
    "time_all = 0    # 消耗的总时长，单位s\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    ts = time.perf_counter() # 打一个时间戳\n",
    "    \n",
    "    train_loss = train_model(epoch, model2_retrain, loss_fn_retrain, optimizer_retrain, benign_trainloader)\n",
    "    val_loss, val_f1, val_recall = val_model(epoch, model2_retrain, loss_fn_retrain, valloader)\n",
    "    print(f\"Epoch {epoch} | TrainLoss {train_loss:.3f} | Val: loss {val_loss:.3f}, f1 {val_f1:.3f}, recall {val_recall:.3f}\")\n",
    "\n",
    "    # 测量模型间相似度\n",
    "    # model1base_cossim = model_cossim(model1, base_model)\n",
    "    model_2retrain_base_cossim = model_cossim(model2_retrain, base_model)\n",
    "    model_1_2retrain_cossim = model_cossim(model1, model2_retrain)\n",
    "    print(f\"model_2retrain_base_cossim: {model_2retrain_base_cossim}, model_1_2retrain_cossim: {model_1_2retrain_cossim}\")\n",
    "    # wandb.log({\"epoch\": epoch, \"model1base_cossim\": model1base_cossim, \"model2base_cossim\": model2base_cossim, \"model12_cossim\": model12_cossim})\n",
    "        \n",
    "    td = time.perf_counter()    # 打一个时间戳 \n",
    "    time_all += (td - ts) \n",
    "    avg_time = time_all / (epoch - start_epoch + 1)\n",
    "    remain_time = (start_epoch + num_epochs - epoch - 1) * avg_time / 60    # 还剩多少时间，单位min\n",
    "    print(f\"Overall | Time {(td - ts):.2f}s, Remain {remain_time:.2f}mins\")\n",
    "    print(\"----- ----- ----- -----\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
