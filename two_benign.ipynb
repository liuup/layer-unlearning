{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 两个相同的模型同时进行训练\n",
    "# 查看一下模型的偏离程度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Subset, DataLoader, TensorDataset\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device count: 1\n",
      "_CudaDeviceProperties(name='Z100SM', major=7, minor=5, total_memory=16368MB, multi_processor_count=64)\n",
      "Computing Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "# 看一下设备数量\n",
    "device_count = torch.cuda.device_count()\n",
    "print(f\"Device count: {device_count}\")\n",
    "for i in range(device_count):\n",
    "    print(torch.cuda.get_device_properties(i))\n",
    "    \n",
    "# cuda计算端\n",
    "computing_device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    computing_device = \"cuda\"\n",
    "print(\"Computing Device: \", computing_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_workers = 8\n",
    "\n",
    "# target_class = 0    # 要修改的标签类别\n",
    "# to_class = 1    # 要修改成什么标签\n",
    "# poison_ratio = 0.05 # 相对于全部的训练数据总量，选择的比例\n",
    "\n",
    "#\n",
    "# 加载所有数据\n",
    "#\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),  # 先四周填充0，在吧图像随机裁剪成32*32\n",
    "    transforms.RandomHorizontalFlip(),  # 数据增强，图像一半的概率翻转，一半的概率不翻转\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),  # R,G,B每层的归一化用到的均值和方差\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# 10个类别，每个类别各5000，共50000\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=False, transform=transform_train)\n",
    "validate_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=False, transform=transform_val)\n",
    "\n",
    "# # 正常数据\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "# 正常数据，用于验证\n",
    "valloader = DataLoader(validate_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "# 正常数据，用于测试\n",
    "# testloader = DataLoader(test_subset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "print(len(trainloader.dataset))\n",
    "print(len(valloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11173962\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "l2_normal = 0.001\n",
    "\n",
    "# 不参与训练，作为基准\n",
    "base_model = torchvision.models.resnet18(pretrained=False, num_classes=10)\n",
    "base_model.conv1 = nn.Conv2d(3, 64, 3, stride=1, padding=1, bias=False)  # 首层改成3x3卷积核\n",
    "base_model.maxpool = nn.MaxPool2d(1, 1, 0)  # 图像太小 本来就没什么特征 所以这里通过1x1的池化核让池化层失效\n",
    "base_model = base_model.to(computing_device)    # 先修改模型 再转移到gpu\n",
    "\n",
    "# 在正常数据上训练\n",
    "model1 = copy.deepcopy(base_model)\n",
    "loss_fn1 = nn.CrossEntropyLoss()\n",
    "optimizer1 = torch.optim.Adam(model1.parameters(), lr=lr, weight_decay=l2_normal)\n",
    "# lr_scheduler1 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer1, mode='min')\n",
    "\n",
    "# 在正常+异常上训练\n",
    "model2 = copy.deepcopy(base_model)\n",
    "loss_fn2 = nn.CrossEntropyLoss()\n",
    "optimizer2 = torch.optim.Adam(model2.parameters(), lr=lr, weight_decay=l2_normal)\n",
    "# lr_scheduler2 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer2, mode='min')\n",
    "\n",
    "\n",
    "if torch.cuda.torch.cuda.device_count() > 1:\n",
    "    model1 = nn.DataParallel(model1)\n",
    "    model2 = nn.DataParallel(model2)\n",
    "\n",
    "\n",
    "print(sum(p.numel() for p in base_model.parameters()))   # 查看一下模型参数量\n",
    "# print(\"Initial lr: \", lr_scheduler1.optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "models = [model1, model2]\n",
    "loss_fns = [loss_fn1, loss_fn2]\n",
    "optimizers = [optimizer1, optimizer2]\n",
    "# lr_schedulers = [lr_scheduler1, lr_scheduler2]\n",
    "\n",
    "trainloaders = [trainloader, trainloader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练\n",
    "def train_model(epoch, model, loss_fn, optimizer, trainloader):\n",
    "    # training\n",
    "    num_batches = len(trainloader)\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch, (X, y) in enumerate(trainloader):\n",
    "        X, y = X.to(computing_device), y.to(computing_device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predict = model(X)\n",
    "\n",
    "        loss = loss_fn(predict, y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    train_loss /= num_batches\n",
    "\n",
    "    return train_loss\n",
    "\n",
    "# 验证\n",
    "def val_model(epoch, model, loss_fn, valloader):\n",
    "    size = len(valloader.dataset)\n",
    "    num_batches = len(valloader)\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    real_labels = []\n",
    "    pre_labels = []\n",
    "    with torch.no_grad():        \n",
    "        for batch, (X, y) in enumerate(valloader):\n",
    "            X, y = X.to(computing_device), y.to(computing_device)\n",
    "\n",
    "            predict = model(X)\n",
    "            loss = loss_fn(predict, y)\n",
    "            val_loss += loss.item()\n",
    "            # val_correct += (predict.argmax(1) == y).type(torch.float).sum().item() \n",
    "            real_labels.extend(y.cpu().numpy())\n",
    "            pre_labels.extend(predict.argmax(1).cpu().numpy())\n",
    "            \n",
    "    val_loss /= num_batches\n",
    "    # val_correct /= size\n",
    "    \n",
    "    f1 = f1_score(real_labels, pre_labels, average='weighted')\n",
    "    recall = recall_score(real_labels, pre_labels, average='weighted')\n",
    "    \n",
    "    # overall_f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    # overall_recall = recall_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "    return val_loss, f1, recall\n",
    "\n",
    "# 测试\n",
    "def test_model(model, loss_fn, testloader):\n",
    "    size = len(testloader.dataset)\n",
    "    \n",
    "    num_batches = len(testloader)\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    real_labels = []    # 真实标签\n",
    "    pre_labels = [] # 预测标签\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, y) in enumerate(testloader):\n",
    "            X, y = X.to(computing_device), y.to(computing_device)\n",
    "            predict = model(X)\n",
    "            loss = loss_fn(predict, y)\n",
    "            test_loss += loss.item()\n",
    "            # test_correct += (predict.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "            # for tmpy in y.cpu().numpy():\n",
    "            real_labels.extend(y.cpu().numpy())\n",
    "            # for tmpp in predict.argmax(1).cpu().numpy():\n",
    "            pre_labels.extend(predict.argmax(1).cpu().numpy())\n",
    "    \n",
    "    test_loss /= num_batches\n",
    "    \n",
    "    return test_loss, real_labels, pre_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测量两个模型间的余弦相似度cossim\n",
    "def model_cossim(model1, model2):\n",
    "    model1_params = torch.cat([p.view(-1) for p in model1.parameters()])\n",
    "    model2_params = torch.cat([p.view(-1) for p in model2.parameters()])\n",
    "    \n",
    "    model1base_cossim = F.cosine_similarity(model1_params.unsqueeze(0), model2_params.unsqueeze(0)).item()\n",
    "    return model1base_cossim\n",
    "\n",
    "# 测量两个模型间的l1距离\n",
    "def model_l1(model1, model2):\n",
    "    pass\n",
    "\n",
    "# 测量两个模型层间的余弦相似度cossim\n",
    "def model_layer_cossim(model1, model2):\n",
    "    pass\n",
    "\n",
    "# 测量两个模型层间的l1距离\n",
    "def model_layer_l1(model1, model2):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "Model 1 | TrainLoss 1.394 | Val: loss 1.192, f1 0.568, recall 0.583\n",
      "Model 2 | TrainLoss 1.395 | Val: loss 1.188, f1 0.580, recall 0.588\n",
      "model1base_cossim: 0.7948256134986877, model2base_cossim: 0.7954664826393127, model12_cossim: 0.9111608862876892\n",
      "Overall | Time 64.43s, Remain 16.11mins\n",
      "----- ----- ----- -----\n",
      "Epoch 2/16\n",
      "Model 1 | TrainLoss 0.958 | Val: loss 1.099, f1 0.625, recall 0.634\n",
      "Model 2 | TrainLoss 0.967 | Val: loss 1.146, f1 0.621, recall 0.619\n",
      "model1base_cossim: 0.663814127445221, model2base_cossim: 0.6640810370445251, model12_cossim: 0.8103222250938416\n",
      "Overall | Time 50.81s, Remain 13.44mins\n",
      "----- ----- ----- -----\n",
      "Epoch 3/16\n",
      "Model 1 | TrainLoss 0.802 | Val: loss 0.793, f1 0.724, recall 0.730\n",
      "Model 2 | TrainLoss 0.808 | Val: loss 1.018, f1 0.655, recall 0.666\n",
      "model1base_cossim: 0.5675181746482849, model2base_cossim: 0.5652353167533875, model12_cossim: 0.6978940963745117\n",
      "Overall | Time 50.87s, Remain 12.00mins\n",
      "----- ----- ----- -----\n",
      "Epoch 4/16\n",
      "Model 1 | TrainLoss 0.708 | Val: loss 1.117, f1 0.622, recall 0.642\n",
      "Model 2 | TrainLoss 0.712 | Val: loss 0.812, f1 0.727, recall 0.732\n",
      "model1base_cossim: 0.4823305904865265, model2base_cossim: 0.4826332628726959, model12_cossim: 0.5980523228645325\n",
      "Overall | Time 51.16s, Remain 10.86mins\n",
      "----- ----- ----- -----\n",
      "Epoch 5/16\n",
      "Model 1 | TrainLoss 0.643 | Val: loss 0.635, f1 0.778, recall 0.782\n",
      "Model 2 | TrainLoss 0.649 | Val: loss 0.862, f1 0.725, recall 0.722\n",
      "model1base_cossim: 0.41314151883125305, model2base_cossim: 0.411722332239151, model12_cossim: 0.5155754685401917\n",
      "Overall | Time 51.20s, Remain 9.84mins\n",
      "----- ----- ----- -----\n",
      "Epoch 6/16\n",
      "Model 1 | TrainLoss 0.586 | Val: loss 0.685, f1 0.760, recall 0.768\n",
      "Model 2 | TrainLoss 0.593 | Val: loss 0.798, f1 0.720, recall 0.720\n",
      "model1base_cossim: 0.36276817321777344, model2base_cossim: 0.36445480585098267, model12_cossim: 0.4601796865463257\n",
      "Overall | Time 50.65s, Remain 8.86mins\n",
      "----- ----- ----- -----\n",
      "Epoch 7/16\n",
      "Model 1 | TrainLoss 0.549 | Val: loss 0.557, f1 0.817, recall 0.816\n",
      "Model 2 | TrainLoss 0.548 | Val: loss 0.649, f1 0.778, recall 0.782\n",
      "model1base_cossim: 0.32469677925109863, model2base_cossim: 0.3275652527809143, model12_cossim: 0.41498059034347534\n",
      "Overall | Time 50.65s, Remain 7.92mins\n",
      "----- ----- ----- -----\n",
      "Epoch 8/16\n",
      "Model 1 | TrainLoss 0.514 | Val: loss 0.719, f1 0.750, recall 0.750\n",
      "Model 2 | TrainLoss 0.516 | Val: loss 0.589, f1 0.804, recall 0.801\n",
      "model1base_cossim: 0.29546037316322327, model2base_cossim: 0.2988283038139343, model12_cossim: 0.3799048066139221\n",
      "Overall | Time 51.03s, Remain 7.01mins\n",
      "----- ----- ----- -----\n",
      "Epoch 9/16\n",
      "Model 1 | TrainLoss 0.489 | Val: loss 0.625, f1 0.793, recall 0.794\n",
      "Model 2 | TrainLoss 0.494 | Val: loss 0.578, f1 0.805, recall 0.804\n",
      "model1base_cossim: 0.2725294232368469, model2base_cossim: 0.27532756328582764, model12_cossim: 0.35149696469306946\n",
      "Overall | Time 51.14s, Remain 6.12mins\n",
      "----- ----- ----- -----\n",
      "Epoch 10/16\n",
      "Model 1 | TrainLoss 0.463 | Val: loss 0.502, f1 0.832, recall 0.832\n",
      "Model 2 | TrainLoss 0.467 | Val: loss 0.526, f1 0.826, recall 0.826\n",
      "model1base_cossim: 0.2545236051082611, model2base_cossim: 0.2578384280204773, model12_cossim: 0.3294825255870819\n",
      "Overall | Time 51.00s, Remain 5.23mins\n",
      "----- ----- ----- -----\n",
      "Epoch 11/16\n",
      "Model 1 | TrainLoss 0.450 | Val: loss 0.592, f1 0.800, recall 0.802\n",
      "Model 2 | TrainLoss 0.453 | Val: loss 0.565, f1 0.814, recall 0.816\n",
      "model1base_cossim: 0.2390255331993103, model2base_cossim: 0.24201849102973938, model12_cossim: 0.3097503185272217\n",
      "Overall | Time 50.91s, Remain 4.35mins\n",
      "----- ----- ----- -----\n",
      "Epoch 12/16\n",
      "Model 1 | TrainLoss 0.431 | Val: loss 0.558, f1 0.817, recall 0.815\n",
      "Model 2 | TrainLoss 0.433 | Val: loss 0.490, f1 0.835, recall 0.836\n",
      "model1base_cossim: 0.22719410061836243, model2base_cossim: 0.22927618026733398, model12_cossim: 0.2954199016094208\n",
      "Overall | Time 50.76s, Remain 3.47mins\n",
      "----- ----- ----- -----\n",
      "Epoch 13/16\n",
      "Model 1 | TrainLoss 0.420 | Val: loss 0.497, f1 0.833, recall 0.836\n",
      "Model 2 | TrainLoss 0.417 | Val: loss 0.494, f1 0.837, recall 0.839\n",
      "model1base_cossim: 0.21622996032238007, model2base_cossim: 0.2192384898662567, model12_cossim: 0.28227514028549194\n",
      "Overall | Time 50.70s, Remain 2.60mins\n",
      "----- ----- ----- -----\n",
      "Epoch 14/16\n",
      "Model 1 | TrainLoss 0.413 | Val: loss 0.486, f1 0.841, recall 0.843\n",
      "Model 2 | TrainLoss 0.410 | Val: loss 0.484, f1 0.841, recall 0.842\n",
      "model1base_cossim: 0.2070322483778, model2base_cossim: 0.20900757610797882, model12_cossim: 0.26961857080459595\n",
      "Overall | Time 50.95s, Remain 1.73mins\n",
      "----- ----- ----- -----\n",
      "Epoch 15/16\n",
      "Model 1 | TrainLoss 0.403 | Val: loss 0.492, f1 0.836, recall 0.838\n",
      "Model 2 | TrainLoss 0.396 | Val: loss 0.457, f1 0.848, recall 0.849\n",
      "model1base_cossim: 0.19867636263370514, model2base_cossim: 0.2010611891746521, model12_cossim: 0.25954800844192505\n",
      "Overall | Time 50.82s, Remain 0.86mins\n",
      "----- ----- ----- -----\n",
      "Epoch 16/16\n",
      "Model 1 | TrainLoss 0.389 | Val: loss 0.505, f1 0.835, recall 0.835\n",
      "Model 2 | TrainLoss 0.391 | Val: loss 0.489, f1 0.840, recall 0.840\n",
      "model1base_cossim: 0.19160665571689606, model2base_cossim: 0.19364742934703827, model12_cossim: 0.24853205680847168\n",
      "Overall | Time 51.11s, Remain 0.00mins\n",
      "----- ----- ----- -----\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 0 # 从哪一个epoch开始\n",
    "num_epochs = 16 # 要训练多少个epoch\n",
    "\n",
    "time_all = 0    # 消耗的总时长，单位s\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch + num_epochs):\n",
    "    ts = time.perf_counter() # 打一个时间戳\n",
    "    \n",
    "    # 训练各个模型\n",
    "    print(f\"Epoch {epoch+1}/{start_epoch + num_epochs}\")\n",
    "    for idx in range(len(models)):\n",
    "        train_loss = train_model(epoch, models[idx], loss_fns[idx], optimizers[idx], trainloaders[idx])\n",
    "        val_loss, val_f1, val_recall = val_model(epoch, models[idx], loss_fns[idx], valloader)\n",
    "        \n",
    "        # lr_schedulers[idx].step(val_loss) # 调整学习率\n",
    "        # now_lr = lr_schedulers[idx].optimizer.param_groups[0][\"lr\"]\n",
    "        \n",
    "        # print(f\"Model {idx} | lr {now_lr} | TrainLoss {train_loss:.3f} | ValLoss {val_loss:.3f} | ValAcc {(val_correct * 100):.2f}\")\n",
    "        print(f\"Model {idx+1} | TrainLoss {train_loss:.3f} | Val: loss {val_loss:.3f}, f1 {val_f1:.3f}, recall {val_recall:.3f}\")\n",
    "\n",
    "        # wandb_data = {\"epoch\": epoch,\n",
    "        #             f\"model{idx+1}_train_loss\": round(train_loss, 5),\n",
    "        #             f\"model{idx+1}_val_loss\": round(val_loss, 5),\n",
    "        #             f\"model{idx+1}_val_f1\": round(val_f1, 5),\n",
    "        #             f\"model{idx+1}_val_recall\": round(val_recall, 5),}\n",
    "        # wandb.log(wandb_data)\n",
    "    \n",
    "\n",
    "    # 测量模型间相似度\n",
    "    model1base_cossim = model_cossim(model1, base_model)\n",
    "    model2base_cossim = model_cossim(model2, base_model)\n",
    "    model12_cossim = model_cossim(model1, model2)\n",
    "    print(f\"model1base_cossim: {model1base_cossim}, model2base_cossim: {model2base_cossim}, model12_cossim: {model12_cossim}\")\n",
    "    # wandb.log({\"epoch\": epoch, \"model1base_cossim\": model1base_cossim, \"model2base_cossim\": model2base_cossim, \"model12_cossim\": model12_cossim})\n",
    "    \n",
    "        \n",
    "    td = time.perf_counter()    # 打一个时间戳 \n",
    "    time_all += (td - ts) \n",
    "    avg_time = time_all / (epoch - start_epoch + 1)\n",
    "    remain_time = (start_epoch + num_epochs - epoch - 1) * avg_time / 60    # 还剩多少时间，单位min\n",
    "    print(f\"Overall | Time {(td - ts):.2f}s, Remain {remain_time:.2f}mins\")\n",
    "    print(\"----- ----- ----- -----\")\n",
    "    \n",
    "    # for idx in range(len(models)):\n",
    "    #     lr_schedulers[idx].step(val_loss) # 调整学习率\n",
    "    # now_lr = lr_scheduler.optimizer.param_groups[0][\"lr\"]\n",
    "    # print(f\"Epoch {epoch+1}/{start_epoch + num_epochs}, Time {(td - ts):.2f}s/{remain_time:.2f}mins | lr {now_lr} | TrainLoss {train_loss:.3f} | ValLoss {val_loss:.3f} | ValAcc {(val_correct * 100):.2f}\")\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
